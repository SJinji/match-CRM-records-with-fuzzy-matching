{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0+lYdeaOUXpsCMmN4FqCu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJinji/match-tables-with-fuzzy-matching/blob/main/Reveal_Tech_Case_Jinji_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fuzzymatcher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R0Xi0cgZ63f",
        "outputId": "8bd7eb11-a3a4-42c8-de71-31e4a64ce111"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzymatcher in /usr/local/lib/python3.10/dist-packages (0.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fuzzymatcher) (1.5.3)\n",
            "Requirement already satisfied: metaphone in /usr/local/lib/python3.10/dist-packages (from fuzzymatcher) (0.6)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (from fuzzymatcher) (0.21.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from fuzzymatcher) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from fuzzymatcher) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fuzzymatcher) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->fuzzymatcher) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->fuzzymatcher) (1.16.0)\n",
            "Requirement already satisfied: Levenshtein==0.21.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein->fuzzymatcher) (0.21.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install recordlinkage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvOt9FYjZ9Qi",
        "outputId": "f4a5936c-bc7b-44d9-c72f-dca478861f07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: recordlinkage in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: jellyfish>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.22.4)\n",
            "Requirement already satisfied: pandas<3,>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1 in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from recordlinkage) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1->recordlinkage) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1->recordlinkage) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3,>=1->recordlinkage) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import fuzzymatcher"
      ],
      "metadata": {
        "id": "S4GEyh2ZZ4cU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "WP0Zx0wVbV6b"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess the datasets\n",
        "def load_and_preprocess_datasets(file_a, file_b):\n",
        "    # Load the datasets into pandas DataFrames\n",
        "    dataset_a = pd.read_csv(file_a, header=None)\n",
        "    dataset_b = pd.read_csv(file_b, header=None)\n",
        "\n",
        "    # Remove column 3 from both datasets\n",
        "    dataset_a.drop(3, axis=1, inplace=True)\n",
        "    dataset_b.drop(3, axis=1, inplace=True)\n",
        "\n",
        "    # Add column names to datasets A and B\n",
        "    column_names = ['id', 'company_name', 'website', 'phone_number', 'address', 'postcode', 'region', 'country']\n",
        "    dataset_a.columns = column_names\n",
        "    dataset_b.columns = column_names\n",
        "\n",
        "    # Preprocess datasets\n",
        "    preprocess_columns(dataset_a)\n",
        "    preprocess_columns(dataset_b)\n",
        "\n",
        "    dataset_a.columns = [f'{col}_a' for col in column_names]\n",
        "    dataset_b.columns = [f'{col}_b' for col in column_names]\n",
        "\n",
        "    return dataset_a, dataset_b"
      ],
      "metadata": {
        "id": "X3a42PfWdWwH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the columns\n",
        "def preprocess_columns(df):\n",
        "    # Convert company names to lowercase and replace \"'\" with whitespace\n",
        "    df['company_name'] = df['company_name'].str.lower().str.replace(\"'\", ' ')\n",
        "\n",
        "    # Remove prefixes \"http://\" and \"www.\" from the website column\n",
        "    df['website'] = df['website'].str.replace(r'^https?://', '', case=False)\n",
        "    df['website'] = df['website'].str.replace(r'^www\\.', '', case=False)\n",
        "    # Remove trailing slashes from the website column\n",
        "    df['website'] = df['website'].str.rstrip('/')\n",
        "\n",
        "    # Remove leading zeros from phone numbers\n",
        "    df['phone_number'] = df['phone_number'].str.replace(r'\\.', '')\n",
        "    df['phone_number'] = df['phone_number'].str.replace(r'^\\+?33|^\\+?0*', '').str.replace(r'\\s', '')\n",
        "\n",
        "    # Remove commas from addresses\n",
        "    df['address'] = df['address'].str.replace(',', '').str.lower()\n",
        "\n",
        "    # Remove \".0\" from postcode and convert it to an integer\n",
        "    df['postcode'] = df['postcode'].astype(str).str.replace(r'\\.0$', '')\n",
        "    df['postcode'] = pd.to_numeric(df['postcode'], errors='coerce')\n",
        "\n",
        "    # Convert region names to lowercase\n",
        "    df['region'] = df['region'].str.lower()\n",
        "\n",
        "    # Convert country names to lowercase\n",
        "    df['country'] = df['country'].str.lower()\n",
        "\n",
        "    # Remove '-' from specified columns\n",
        "    columns_to_remove_dash = ['company_name', 'address', 'region', 'country']\n",
        "    for col in columns_to_remove_dash:\n",
        "        df[col] = df[col].str.replace('-', '').str.strip()\n",
        "\n",
        "    # Remove duplicates from the DataFrame\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "olbdWAXMdbWo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find common IDs based on different attributes\n",
        "def find_common_ids(dataset_a, dataset_b):\n",
        "    # Find common ids based on id, website, or phone_number\n",
        "    common_ids_id = dataset_a.merge(dataset_b, left_on='id_a', right_on='id_b', how='inner')\n",
        "\n",
        "    # Filter out rows with non-null values in the \"website_a\" column\n",
        "    dataset_a_website_notnull = dataset_a.dropna(subset=['website_a'])\n",
        "    common_ids_website = dataset_a_website_notnull.merge(dataset_b, left_on='website_a', right_on='website_b', how='inner')\n",
        "\n",
        "    # Filter out rows with non-null values in the \"phone_number_a\" column\n",
        "    dataset_a_phone_notnull = dataset_a.dropna(subset=['phone_number_a'])\n",
        "    common_ids_phone = dataset_a_phone_notnull.merge(dataset_b, left_on='phone_number_a', right_on='phone_number_b', how='inner')\n",
        "\n",
        "    # Concatenate all common_ids based on different criteria to get unique common ids\n",
        "    common_ids = pd.concat([common_ids_id, common_ids_website, common_ids_phone]).drop_duplicates()\n",
        "\n",
        "    return common_ids"
      ],
      "metadata": {
        "id": "TaqjpJi3d94G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to exclude common_ids and get unmatched rows\n",
        "def exclude_common_ids(dataset_a, dataset_b, common_ids):\n",
        "    dataset_a_only = dataset_a[~dataset_a['id_a'].isin(common_ids['id_a'])]\n",
        "    dataset_b_only = dataset_b[~dataset_b['id_b'].isin(common_ids['id_b'])]\n",
        "\n",
        "    # Fill NaN values with empty strings in dataset_a and dataset_b\n",
        "    dataset_a_only.fillna('', inplace=True)\n",
        "    dataset_b_only.fillna('', inplace=True)\n",
        "\n",
        "    return dataset_a_only, dataset_b_only"
      ],
      "metadata": {
        "id": "uOUUUSC1eokN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform fuzzy matching\n",
        "def perform_fuzzy_matching(dataset_a_only, dataset_b_only):\n",
        "    left_on = [\"company_name_a\", 'website_a', 'phone_number_a', \"address_a\", 'postcode_a', 'region_a', 'country_a']\n",
        "    right_on = [\"company_name_b\", 'website_b', 'phone_number_b', \"address_b\", 'postcode_b', 'region_b', 'country_b']\n",
        "\n",
        "    # Running time: 1min\n",
        "    matched_results = fuzzymatcher.fuzzy_left_join(dataset_a_only,\n",
        "                                                   dataset_b_only,\n",
        "                                                   left_on,\n",
        "                                                   right_on,\n",
        "                                                   left_id_col='id_a',\n",
        "                                                   right_id_col='id_b')\n",
        "\n",
        "    # Filter out rows with NaN in 'best_match_score' column\n",
        "    matched_results = matched_results[matched_results['best_match_score'].notna()]\n",
        "\n",
        "    # Convert 'id_b' to integer data type\n",
        "    matched_results['id_b'] = matched_results['id_b'].astype(int)\n",
        "\n",
        "    return matched_results"
      ],
      "metadata": {
        "id": "J_EBtQwpe23D"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter matched_results and concatenate with common_ids\n",
        "def filter_and_concat(matched_results, common_ids):\n",
        "    cols = [\n",
        "        \"best_match_score\", 'id_a', \"company_name_a\", 'website_a', 'phone_number_a', \"address_a\", 'postcode_a', 'region_a',\n",
        "        'country_a', 'id_b', \"company_name_b\", 'website_b', 'phone_number_b', \"address_b\", 'postcode_b', 'region_b', 'country_b'\n",
        "    ]\n",
        "\n",
        "    # Sort the results by best_match_score in descending order\n",
        "    matched_results = matched_results[cols].sort_values(by=['best_match_score'], ascending=False)\n",
        "\n",
        "    # Filter matched_results based on best_match_score >= -0.01\n",
        "    filtered_matched_results = matched_results[matched_results['best_match_score'] >= -0.01]\n",
        "\n",
        "    # Concatenate filtered_matched_results with common_ids\n",
        "    final_result = pd.concat([filtered_matched_results, common_ids], axis=0)\n",
        "\n",
        "    return final_result\n"
      ],
      "metadata": {
        "id": "r3IO74hsfA26"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the result as a CSV file\n",
        "def save_to_csv(final_result, output_file):\n",
        "    # Keep only the desired columns\n",
        "    final_result = final_result[['id_a', 'company_name_a', 'id_b', 'company_name_b', 'best_match_score']]\n",
        "\n",
        "    # Save the result as a CSV file\n",
        "    final_result.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "rKECdOSOfJjK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Load and preprocess datasets\n",
        "    dataset_a, dataset_b = load_and_preprocess_datasets('dataset_A.csv', 'dataset_B.csv')\n",
        "\n",
        "    # Find common IDs based on different attributes\n",
        "    common_ids = find_common_ids(dataset_a, dataset_b)\n",
        "\n",
        "    # Exclude common_ids and get unmatched rows\n",
        "    dataset_a_only, dataset_b_only = exclude_common_ids(dataset_a, dataset_b, common_ids)\n",
        "\n",
        "    # Perform fuzzy matching\n",
        "    matched_results = perform_fuzzy_matching(dataset_a_only, dataset_b_only)\n",
        "\n",
        "    # Filter and concatenate matched_results with common_ids\n",
        "    final_result = filter_and_concat(matched_results, common_ids)\n",
        "\n",
        "    # Save the result as a CSV file\n",
        "    save_to_csv(final_result, 'matched_results.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "IgIPrIdOfM4s"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}